{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import json\n",
    "\n",
    "folder = 'CL_Cup IT_Data_Scince_секция_кейс_VK_датасет.zip'\n",
    "test_data = db.read_text(\n",
    "    'zip://ranking_test.jsonl',\n",
    "    storage_options={'fo': folder},\n",
    "    encoding='Windows-1251'\n",
    ").map(json.loads)\n",
    "train_data = db.read_text(\n",
    "    'zip://ranking_train.jsonl',\n",
    "    storage_options={'fo': folder},\n",
    "    encoding='Windows-1251'\n",
    ").map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_data.to_dataframe().compute()\n",
    "train_df = train_data.to_dataframe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>I&amp;#x27;m still waiting for them to stabilize w...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>For those who upgraded, no need to do a restor...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>Upgraded shortly after it was released and suf...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>I think they were under a lot of pressure on t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>Fix for those who already updated:  http:&amp;#x2F...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  iOS 8.0.1 released, broken on iPhone 6 models,...   \n",
       "0  iOS 8.0.1 released, broken on iPhone 6 models,...   \n",
       "0  iOS 8.0.1 released, broken on iPhone 6 models,...   \n",
       "0  iOS 8.0.1 released, broken on iPhone 6 models,...   \n",
       "0  iOS 8.0.1 released, broken on iPhone 6 models,...   \n",
       "\n",
       "                                             comment score  \n",
       "0  I&#x27;m still waiting for them to stabilize w...  None  \n",
       "0  For those who upgraded, no need to do a restor...  None  \n",
       "0  Upgraded shortly after it was released and suf...  None  \n",
       "0  I think they were under a lot of pressure on t...  None  \n",
       "0  Fix for those who already updated:  http:&#x2F...  None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = test_df.explode(column='comments')\n",
    "test['comment'] = test.comments.map(lambda dic: dic['text'])\n",
    "test['score'] = test.comments.map(lambda dic: dic['score'])\n",
    "test.drop(labels=['comments'], axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many summer Y Combinator fundees decided n...</td>\n",
       "      <td>Going back to school is not identical with giv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many summer Y Combinator fundees decided n...</td>\n",
       "      <td>There will invariably be those who don't see t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many summer Y Combinator fundees decided n...</td>\n",
       "      <td>For me school is a way to be connected to what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many summer Y Combinator fundees decided n...</td>\n",
       "      <td>I guess it really depends on how hungry you ar...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many summer Y Combinator fundees decided n...</td>\n",
       "      <td>I know pollground decided to go back to school...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  How many summer Y Combinator fundees decided n...   \n",
       "0  How many summer Y Combinator fundees decided n...   \n",
       "0  How many summer Y Combinator fundees decided n...   \n",
       "0  How many summer Y Combinator fundees decided n...   \n",
       "0  How many summer Y Combinator fundees decided n...   \n",
       "\n",
       "                                             comment  score  \n",
       "0  Going back to school is not identical with giv...      0  \n",
       "0  There will invariably be those who don't see t...      1  \n",
       "0  For me school is a way to be connected to what...      2  \n",
       "0  I guess it really depends on how hungry you ar...      3  \n",
       "0  I know pollground decided to go back to school...      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_df.explode(column='comments')\n",
    "train['comment'] = train.comments.map(lambda dic: dic['text'])\n",
    "train['score'] = train.comments.map(lambda dic: dic['score'])\n",
    "train.drop(labels=['comments'], axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class preprocessingWrapper:\n",
    "\n",
    "  \"\"\"\n",
    "    Класс, в рамках которого происходит препроцессинг данных\n",
    "    для последующих шагов, связанных с EDA и ML. Датафрейм\n",
    "    оборачивается этим классом и в результате обертки\n",
    "    возвращается обработанный датасет\n",
    "  \"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def pos_tagger(treebank_tag: str):\n",
    "    \"\"\" Возвращает Part-Of-Speech, относящийся к конкретному значению treebank_tag \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "  def __init__(self):\n",
    "    self.stemmer = PorterStemmer()\n",
    "    self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    self.spec_chars = re.compile(r\"[$&+,:;=?@#|'<>.-^*()%!]\")\n",
    "    self.unicode_chars = re.compile(\n",
    "      \"[\"\n",
    "      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U000024C2-\\U0001F251\"\n",
    "      u\"\\U0001f926-\\U0001f937\"\n",
    "      u\"\\U00010000-\\U0010ffff\"\n",
    "      u\"\\u2640-\\u2642\" \n",
    "      u\"\\u2600-\\u2B55\"\n",
    "      u\"\\u200d\"\n",
    "      u\"\\u23cf\"\n",
    "      u\"\\u23e9\"\n",
    "      u\"\\u231a\"\n",
    "      u\"\\ufe0f\"  # dingbats\n",
    "      u\"\\u3030\"\n",
    "      u\"\\u2705\"\n",
    "      \"]+\", \n",
    "      re.UNICODE\n",
    "    )\n",
    "    self.stop_words = re.compile(\n",
    "      r'\\b(?:'+'|'.join(set(stopwords.words('english'))).rstrip('|')+')\\b'\n",
    "    )\n",
    "    \n",
    "  def stem(self, tokens):\n",
    "    return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "  def lemma(self, tokens):\n",
    "    return [\n",
    "      (self.wordnet_lemmatizer.lemmatize(\n",
    "        token, \n",
    "        pos=self.pos_tagger(nltk.pos_tag([token])[0][1])\n",
    "      ) if self.pos_tagger(nltk.pos_tag([token])[0][1])\n",
    "      else self.wordnet_lemmatizer.lemmatize(token)) \n",
    "      if token.isalpha() else token\n",
    "      for token in tokens \n",
    "    ]\n",
    "\n",
    "  def __call__(\n",
    "    self, df: pd.DataFrame, column_for_cleaning: str, op: str\n",
    "  ):\n",
    "    \"\"\" \n",
    "      Этот основной метод позволяет 'обернуть' датафрейм и очистить столбец \n",
    "      \n",
    "      df: pd.DataFrame\n",
    "        - Датафрейм для очистки\n",
    "      column_for_cleaning: str\n",
    "        - Столбец для очистки\n",
    "      op: str ('stem'/'lemma')\n",
    "        - Какую операцию в итоге провести над токенами    \n",
    "    \"\"\"\n",
    "    if op not in ['stem', 'lemma']:\n",
    "      raise Exception(\n",
    "        'op принимает значения \"stem\" или \"lemma\"'\n",
    "      )\n",
    "\n",
    "    column = df[column_for_cleaning]\n",
    "    column = column.str.lower()\n",
    "\n",
    "    # удаление спец. символов\n",
    "    column = column.str.replace(self.spec_chars, '', regex=True) \n",
    "\n",
    "    # удаление других юникодных символов\n",
    "    column = column.str.replace(self.unicode_chars, '', regex=True) \n",
    "    \n",
    "    # удаление стоп-слов\n",
    "    column = column.str.replace(self.stop_words, '', regex=True)\n",
    "\n",
    "    # токенизация\n",
    "    column = column.apply(word_tokenize, meta=('comment', 'object'))\n",
    "\n",
    "    if op == 'stem':\n",
    "      # стемматизация\n",
    "      return column.apply(self.stem, meta=('comment', 'object'))\n",
    "    else:\n",
    "      # лемматизация\n",
    "      return column.apply(self.lemma, meta=('comment', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "\n",
    "preprocesser = preprocessingWrapper()\n",
    "trained_ddf = ddf.from_pandas(\n",
    "    train, npartitions=12\n",
    ")\n",
    "trained_posts_lemmatized = preprocesser(trained_ddf, 'comment', 'lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 2.73 s ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\GitHub\\CUP_IT\\cupit_v1.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/GitHub/CUP_IT/cupit_v1.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trained_posts_lemmatized \u001b[39m=\u001b[39m trained_posts_lemmatized\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\site-packages\\dask\\base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    292\u001b[0m     \u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[0;32m    294\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39m, traverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\site-packages\\dask\\base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[0;32m    598\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 600\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[39m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[39m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[39m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\site-packages\\dask\\local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[1;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[0;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\site-packages\\dask\\local.py:130\u001b[0m, in \u001b[0;36mqueue_get\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m         \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, timeout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    131\u001b[0m     \u001b[39mexcept\u001b[39;00m Empty:\n\u001b[0;32m    132\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_posts_lemmatized = trained_posts_lemmatized.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
